---
permalink: admin/grid-federation-overview.html
sidebar: sidebar
keywords: grid federation, cross-grid replication, account clone, cgr, disaster recovery
summary: 'You can use grid federation to clone tenants and replicate their objects between two StorageGRID systems for disaster recovery.'
---
= Use grid federation: Overview
:icons: font
:imagesdir: ../media/

[.lead]
You can use grid federation to clone tenants and replicate their objects between two StorageGRID systems for disaster recovery.

== What is a grid federation connection?

A grid federation connection is a bidirectional, trusted, and secure connection between Admin and Gateway Nodes in two StorageGRID systems.

=== Considerations and requirements for grid federation connections

* Both grids used for grid federation must be running StorageGRID 11.7.

* A grid can have one or more grid federation connections to other grids. Each grid federation connection is independent of any other connections. That is, if grid 1 has one connection with grid 2 and a second connection with grid 3, there is no implied connection between grid 2 and grid 3.

* Grid federation connections are bidirectional. After the connection is established, you can monitor and manage the connection from either grid. 

* At least one grid federation connection must exist before you can use <<account-clone,account clone>> or <<cross-grid-replication,cross-grid replication>>.

==== IP address and DNS requirements

* A grid federation connection can connect any two nodes that run the nginx-gw service. All Admin Nodes and all Gateway Nodes include this service, so you can configure a connection between the primary Admin Nodes on each grid or between a Gateway Node on one grid and an Admin Node on the other. However, the best practice is to connect high availability (HA) groups of Gateway and Admin Nodes on each grid.
+
Using HA groups helps ensure that grid federation connections will remain online if nodes become unavailable. If the active interface in either HA group fails, the connection can use a backup interface. For details, see xref:managing-high-availability-groups.adoc[Manage high availability (HA) groups].

* If you plan to use fully qualified domain names (FQDN) to identify each grid in the connection (recommended), you must create the appropriate DNS entries, as follows:

** *Grid 1 DNS*: The FQDN for Grid 2 mapped to one or more IP address or virtual IP (VIP) addresses for Grid 1. 
** *Grid 2 DNS*: The FQDN for Grid 1 mapped to one or more IP addresses or VIP addresses for Grid 2. 

* If the DNS entries for a FQDN map to the IP address of more than one Admin Node or Gateway Node or to the VIP addresses of more than one HA group, the requests to use the connection will be load balanced between the nodes or between the active nodes in the HA groups.

==== Port requirements

When creating a grid federation connection, you can specify any unused port number from 23000 to 23999. Both grids in this connection will use the same port. You must ensure that no node in either grid uses this port for other connections.

==== Certificate requirements

When you configure a grid federation connection, StorageGRID automatically generates four SSL certificates:

* Server and client certificates to authenticate and encrypt the connection between grid 1 and grid 2
* Server and client certificates to authenticate and encrypt the connection between grid 2 and grid 1

image:../media/grid-federation-certificates.png[Grid federation certificates]

By default, the certificates are valid for 730 days (2 years). When these certificates near their expiration date, 
the *Expiration of grid federation certificate* alert reminds you to rotate the certificates, which you can do using the Grid Manager. 

[IMPORTANT]
If the certificates on either end of the connection expire, the connection will stop working and data will no longer be replicated between grids. 

.Learn more
* xref:grid-federation-create-connection.adoc[Create grid federation connections]
* xref:grid-federation-manage-connection.adoc[Manage grid federation connections]

== [[account-clone]]What is account clone?

Account clone is the replication of tenant accounts, tenant groups, tenant users, and, optionally, S3 access keys between a source StorageGRID system and a destination StorageGRID system. Account clone is required for cross-grid replication. Cloning account information from a source grid to a destination grid ensures that tenant users and groups can access the corresponding buckets and objects on either grid.

=== How are tenant accounts cloned?

After creating a grid federation connection, a grid admin can create new S3 tenant accounts on either grid. Tenants that have the *Use grid federation connection* permission can be allowed to use one or more grid federation connections. When the new tenant is saved, StorageGRID automatically creates a replica of that tenant on the other grid.

The grid where the tenant is originally created is known as the tenant's _source grid_. The grid where the tenant is replicated to is known as the tenant's _destination grid_. Both tenant accounts have the same account ID, name, storage quota, and assigned permissions, but the tenant replicated to the destination grid has fewer capabilities and does not initially have a root user password.

NOTE: You can only add the *Use grid federation connection* when you are creating a new S3 tenant; you can't add this permission to an existing tenant.

.Learn more:
xref:grid-federation-manage-tenants.adoc[Manage permitted tenants].

=== How are groups, users, and S3 access keys cloned?

After tenants have been created and replicated, new groups and users are automatically cloned from the tenant's source grid to the tenant's destination grid.

image:../media/grid-federation-account-clone.png[image showing that tenant details are cloned from source grid to destination grid]

As indicated by the figure:

* Assuming the <<account-clone-identity-federation,requirements for identity federation>> have been met, federated groups imported to the tenant on the source grid are automatically imported to the tenant on the destination grid. The group permissions and S3 policy are the same.

* If a federated group has been imported to the source grid, federated users will be allowed to access the source grid. However, the federated user's S3 access keys must be cloned to the destination grid before that user will be allowed to access the destination grid.

* Local tenant groups and users created on the source grid are automatically cloned to the destination grid.

* Optionally, S3 access keys created on the source grid can be manually cloned to the destination grid.
+ 
For security reasons, StorageGRID does not automatically clone S3 access keys from the tenant's source grid to the destination grid. If tenant users need to access the buckets on both grids, you must use the Tenant Manager API to manually copy the keys from the source grid to the destination grid. See xref:../tenant/grid-federation-clone-keys-with-api.adoc[Clone S3 access keys using the API].


Note that account cloning occurs in one direction only. If you create groups, users, and S3 access keys on the tenant's destination grid, these items are not cloned back to the source grid. 

image:../media/grid-federation-account-not-cloned.png[image showing that tenant details are cloned back to source grid]


Also note that any edits made to the tenant, users, groups, or access keys on either grid are not cloned to the other grid. 

=== [[account-clone-identity-federation]]Considerations and requirements for SSO and identity federation

* If either StorageGRID system in the connection uses single sign-on (SSO), the grid administrator must configure the same identity source and the same SSO identity provider (IdP) for both grids.

* If either StorageGRID system in the connection uses identity federation (but not SSO), the grid admin user must configure the same identity source for both grids. Tenant accounts that have the *Use own identity source* permission must configure the same identity source for both the source and destination tenant accounts. 

.Learn more

xref:../tenant/grid-federation-use-account-clone.adoc[Use account clone]

== [[cross-grid-replication]]What is cross-grid replication?

Cross-grid replication is the automatic replication of objects between selected S3 buckets in two StorageGRID systems that are connected in a grid federation connection.

=== Requirements for cross-grid replication

If a tenant account has the *Use grid federation connection* permission, a tenant user with Root Access permission can create identical buckets in the corresponding tenant accounts on each grid. These buckets:

* Must have the same name and region
* Must have versioning enabled
* Must have S3 Object Lock disabled
* Must be empty

.Learn more

xref:../tenant/grid-federation-manage-cross-grid-replication.adoc[Manage cross-grid replication
]

=== How cross-grid replication works

After both buckets have been created, cross-grid replication can be configured to occur in one direction or in both directions.

==== [[replication-one-direction]]Replication in one direction

If you enable cross-grid replication for a bucket on only one grid, objects added to the source bucket are replicated to the destination bucket, but objects added to the destination bucket are not replicated back to the source. In the figure, cross-grid replication is enabled for `my-bucket` from Grid 1 to Grid 2, but it is not enabled in the other direction. 

image:../media/grid-federation-cross-grid-replication-one-direction.png[image showing grid federation connection in one direction]

==== [[replication-both-directions]]Replication in both directions
If you enable cross-grid replication for the same bucket on both grids, objects added to either bucket are replicated to the other grid. In the figure, cross-grid replication is enabled for `my-bucket` in both directions. 

image:../media/grid-federation-cross-grid-replication.png[image showing replication in one direction vs replication in both directions]

==== [[client-writes]]What happens when objects are added

When an S3 client adds an object to a bucket that has cross-grid replication enabled, that object version is immediately replicated from the source bucket to the destination bucket. The object is then stored according to the matching ILM rule in each grid's active ILM policy. For example, object A on grid 1 might be stored as two replicated copies and retained forever, while the copy of object A that was replicated to grid 2 might be stored using 2+1 erasure coding and deleted after three years. 

==== [[client-deletes]]What happens when objects are deleted

To understand what happens when an S3 client deletes objects from a bucket that has cross-grid replication enabled, start by reviewing how S3 clients delete objects from buckets that have versioning enabled, as follows:

* If an S3 client issues a delete request that includes a version ID, that version of the object is permanently removed. No delete marker is added the bucket. 

* If an S3 client issues a delete request that does not include a version ID, StorageGRID does not delete any object versions. Instead, it adds a delete marker to the bucket. The delete marker causes StorageGRID to behave as if the object was deleted:

** A GET request without a version ID will fail with `404 No Object Found`
** A GET request with a valid version ID will succeed and return the requested object version.

When you enable cross-grid replication for a bucket, you can specify what happens when S3 clients issue a delete request that does not include a version ID:

* If you choose to replicate delete markers, a delete marker is added to the source bucket and replicated to the destination bucket. In effect, the objects appear to be deleted on both grids.

*  If you choose not to replicate delete markers, a delete marker is added to the source bucket, but it is not replicated to the destination bucket. In effect, objects that the client deletes on the source grid are not deleted on the destination grid.

IMPORTANT: The option to replicate client delete markers does not affect delete object requests that include a version ID. When you include a version ID, objects are permanently removed from the source grid. However, these requests are never replicated to the destination grid because they do not add delete markers to the bucket. 

In the figure, the grids are configured for cross-grid replication in both directions, but delete markers are not replicated from `my-bucket` on Grid 1 to `my-bucket` on Grid 2.

image:../media/grid-federation-cross-grid-replication-delete.png[image showing replicate client delete on both grids]

==== How encrypted objects are replicated
When you use cross-grid replication to replicate objects between grids, you can encrypt individual objects, use default bucket encryption, or configure grid-wide encryption. You can add, modify, or remove encryption settings before or after you enable cross-grid replication for a bucket.

To encrypt individual objects, you can use SSE (server-side encryption with StorageGRID-managed keys) when adding the objects to the source bucket. Use the `x-amz-server-side-encryption` request header and specify `AES256`. See xref:../s3/using-server-side-encryption.adoc[Use server-side encryption]. 

NOTE: Using SSE-C (server-side encryption with customer-provided keys) is not supported for cross-grid replication. The ingest operation will fail.

To use default encryption for a bucket, use a PUT bucket encryption request and set the `SSEAlgorithm` parameter to `AES256`. Bucket-level encryption applies to objects that are not encrypted at the object level during ingest. See xref:../s3/operations-on-buckets.adoc[Operations on buckets]. 

To use grid-level encryption, set the *Stored object encryption* option to *AES-256*. Grid-level encryption applies to any objects that are not encrypted at the bucket level or at the object level during ingest. See xref:../admin/changing-network-options-object-encryption.adoc[Configure network and object options].

NOTE: SSE does not support AES128. If *Stored object encryption* option is enabled for the source grid using the *AES-128* option, objects replicated to the destination grid will not be encrypted.

When encryption is enabled, StorageGRID uses these rules to determine whether to encrypt the replicated objects at the destination:

* If a source object uses SSE, the object replicated to the destination bucket will also use SSE.  
* If a source object does not use SSE, the object replicated to the destination bucket will not be encrypted, unless the destination bucket or grid has encryption configured. In that case, the destination bucket or grid's default encryption is applied to the replicated object.

* If the *Stored object encryption* option is enabled for the source grid using the *AES-256* option, objects replicated to the destination grid will also be encrypted.

* If the *Stored object encryption* option is enabled only for the destination grid, objects replicated to the destination will be encrypted.

==== How segmented objects are replicated

The source grid's maximum segment size applies to objects replicated to the destination grid. When objects are replicated to another grid, the *Maximum Segment Size* setting (*Configuration* > *System* > *Storage options*) of the source grid will be used on both grids. For example, suppose the maximum segment size for the source grid is 1 GB, while the maximum segment size of the destination grid is 50 MB. If you ingest a 2-GB object on the source grid, that object is saved as two 1-GB segments. It will also be replicated to the destination grid as two 1-GB segments, even though that grid's maximum segment size is 50 MB. 




